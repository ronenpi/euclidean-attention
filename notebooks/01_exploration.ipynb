{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Euclidean Approximate Attention - Exploration Notebook\n",
        "\n",
        "This notebook provides interactive exploration of the key concepts:\n",
        "1. Distance matrix approximation\n",
        "2. Euclidean vs Dot-Product attention\n",
        "3. Linear-time approximation quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device setup\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding Distance Matrices\n",
        "\n",
        "A distance matrix D[i,j] = ||x_i - x_j||Â² captures pairwise distances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from distance_estimators import compute_squared_euclidean_distance_matrix, LowRankDistanceApproximator\n",
        "\n",
        "# Generate some random embeddings\n",
        "n, d = 64, 32  # 64 tokens, 32 dimensions\n",
        "X = torch.randn(n, d)\n",
        "\n",
        "# Compute exact distance matrix\n",
        "D = compute_squared_euclidean_distance_matrix(X)\n",
        "\n",
        "print(f'X shape: {X.shape}')\n",
        "print(f'D shape: {D.shape}')\n",
        "print(f'D min: {D.min():.4f}, D max: {D.max():.4f}')\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(D.numpy(), cmap='viridis')\n",
        "plt.colorbar()\n",
        "plt.title('Squared Euclidean Distance Matrix')\n",
        "plt.xlabel('Token j')\n",
        "plt.ylabel('Token i')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Low-Rank Approximation\n",
        "\n",
        "Key insight from Indyk et al.: Distance matrices can be approximated with sublinear samples!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different ranks\n",
        "ranks = [4, 8, 16, 32]\n",
        "errors = []\n",
        "\n",
        "fig, axes = plt.subplots(1, len(ranks) + 1, figsize=(4 * (len(ranks) + 1), 3))\n",
        "\n",
        "# Original\n",
        "axes[0].imshow(D.numpy(), cmap='viridis')\n",
        "axes[0].set_title('Exact D')\n",
        "axes[0].axis('off')\n",
        "\n",
        "for i, rank in enumerate(ranks):\n",
        "    approx = LowRankDistanceApproximator(rank=rank, epsilon=0.1)\n",
        "    D_approx = approx(X) ** 2  # Square since method returns distances\n",
        "    \n",
        "    rel_error = torch.norm(D - D_approx, 'fro') / torch.norm(D, 'fro')\n",
        "    errors.append(rel_error.item())\n",
        "    \n",
        "    axes[i + 1].imshow(D_approx.numpy(), cmap='viridis')\n",
        "    axes[i + 1].set_title(f'Rank {rank}\\nError: {rel_error:.3f}')\n",
        "    axes[i + 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Errors by rank: {dict(zip(ranks, errors))}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Attention Mechanisms Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from attention import StandardAttention, EuclideanAttention, ApproximateEuclideanAttention\n",
        "\n",
        "# Create input\n",
        "batch_size, seq_len, embed_dim = 1, 32, 64\n",
        "num_heads = 4\n",
        "X = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Initialize attention\n",
        "std_attn = StandardAttention(embed_dim, num_heads)\n",
        "euc_attn = EuclideanAttention(embed_dim, num_heads)\n",
        "approx_attn = ApproximateEuclideanAttention(embed_dim, num_heads, num_landmarks=8)\n",
        "\n",
        "# Share weights\n",
        "with torch.no_grad():\n",
        "    euc_attn.q_proj.weight.copy_(std_attn.q_proj.weight)\n",
        "    euc_attn.k_proj.weight.copy_(std_attn.k_proj.weight)\n",
        "    euc_attn.v_proj.weight.copy_(std_attn.v_proj.weight)\n",
        "    approx_attn.q_proj.weight.copy_(std_attn.q_proj.weight)\n",
        "    approx_attn.k_proj.weight.copy_(std_attn.k_proj.weight)\n",
        "    approx_attn.v_proj.weight.copy_(std_attn.v_proj.weight)\n",
        "\n",
        "# Compute attention\n",
        "with torch.no_grad():\n",
        "    out_std, attn_std = std_attn(X, return_attention=True)\n",
        "    out_euc, attn_euc = euc_attn(X, return_attention=True)\n",
        "    out_approx, _ = approx_attn(X)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "axes[0].imshow(attn_std[0, 0].numpy(), cmap='viridis')\n",
        "axes[0].set_title('Standard (Dot-Product) Attention')\n",
        "axes[1].imshow(attn_euc[0, 0].numpy(), cmap='viridis')\n",
        "axes[1].set_title('Euclidean Attention')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare outputs\n",
        "cos_sim = F.cosine_similarity(out_std.flatten().unsqueeze(0), out_euc.flatten().unsqueeze(0)).item()\n",
        "print(f'Output cosine similarity (std vs euc): {cos_sim:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
